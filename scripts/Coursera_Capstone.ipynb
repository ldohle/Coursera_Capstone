{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Accident Severity (Week 2)\n",
    "### Applied Data Science Capstone by IBM/Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Introduction: Business Problem](#introduction)\n",
    "* [Data and data preprocessing](#data)\n",
    "* [Methodology](#methodology)\n",
    "* [Discussion](#discussion)\n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Business Problem <a name=\"introduction\"></a>\n",
    "### 1.1 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2018, the United States had roughly 276 million vehicles in operation.  Out of those vehicles, 12 million were involved in crashes.  The United States is also among the countries with the highest rate of traffic-related fatalities per one million population.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Problem\n",
    "A lot of these accidents could be prevented if the drivers were more informed about the possible severity of car accident based on different factors. The focus of this project therefore is to predict the severity of an accident, using a number of features. For performance reasons, we will focus our efforts on accidents in the Seattle area, but this will allow us to draw conclusions which are relevant for other areas in the US as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Interest\n",
    "Reliable severity prediction of possible car accidents can help commuters to drive with more foresight or change their travel plans accordingly. In addition, an accident severity prediction system can be interesting for insurance companies in order to prevent possible costs for accidents suffered by their members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data and data preprocessing <a name=\"data\"></a>\n",
    "### 2.1 Data Source\n",
    "\n",
    "The dataset was obtained from the [Seattle Department of Transportation website](https://data.seattle.gov/Land-Base/Collisions/9kas-rb8d \"Seattle Department of Transportation website\"). The data was collected from 2004 to present for the Seattle area. The number of observations in the data is about 220,000 and each observation represents a traffic collision. The original data contains our chosen label, i.e a severity code based on the severity of a collision, where 1 indicates just a property damage and 4 indicates a fatality as highest impact of the collision. It also consists of 39 attributes.\n",
    "\n",
    "Before we get started loading the dataset, let's import all needed libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import needed libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timezonefinder import TimezoneFinderL\n",
    "from scipy import stats\n",
    "\n",
    "# we are using the inline backend\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Import train test split and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (35) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read the file\n",
    "path = \"https://raw.githubusercontent.com/ldohle/Coursera_Capstone/master/datasets/Collisions.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data cleaning\n",
    "\n",
    "There were several steps necessary to clean the dataset and make it usable for the problem statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop irrelevant attributes**\n",
    "\n",
    "First, not all of the attributes in the dataset are relevant for the problem statement. Some are also obvious information duplicates compared to other columns.\n",
    "\n",
    "Therefore 11 features were removed from the report entirely: \n",
    "\n",
    "* OBJECTID, INCKEY, SDOTCOLNUM and COLDETKEY were just different variations of unique identifiers, not important to the problem statement at hand.\n",
    "* SDOT_COLDESC was a description of the collision, that directly duplicated the information given via SDOT_COLCODE. \n",
    "* ST_COLDESC was a description of the collision, that directly duplicated the information given via ST_COLCODE.\n",
    "* EXCEPTRSNCODE and EXCEPTRSNDESC was not described in the metadata and also only consisted of different forms of missing data.\n",
    "* INCDATE was a duplicate of information also present in INCDTTM.\n",
    "* STATUS and REPORTNO we removed since it they were not described in the metadata.\n",
    "\n",
    "This left us with 28 attributes still present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns not relevant to our problem statement\n",
    "\n",
    "df_new=df.drop(columns=['OBJECTID', 'INCKEY','COLDETKEY', 'SDOT_COLDESC', 'SDOTCOLNUM', 'ST_COLDESC', 'REPORTNO', 'STATUS', 'EXCEPTRSNCODE', 'EXCEPTRSNDESC', 'INCDATE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change columns to correct data format**\n",
    "\n",
    "The next step in data cleaning was to check and make sure that all data was in the correct format (int, float, text or other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X                  float64\n",
       "Y                  float64\n",
       "ADDRTYPE            object\n",
       "INTKEY             float64\n",
       "LOCATION            object\n",
       "SEVERITYCODE        object\n",
       "SEVERITYDESC        object\n",
       "COLLISIONTYPE       object\n",
       "PERSONCOUNT          int64\n",
       "PEDCOUNT             int64\n",
       "PEDCYLCOUNT          int64\n",
       "VEHCOUNT             int64\n",
       "INJURIES             int64\n",
       "SERIOUSINJURIES      int64\n",
       "FATALITIES           int64\n",
       "INCDTTM             object\n",
       "JUNCTIONTYPE        object\n",
       "SDOT_COLCODE       float64\n",
       "INATTENTIONIND      object\n",
       "UNDERINFL           object\n",
       "WEATHER             object\n",
       "ROADCOND            object\n",
       "LIGHTCOND           object\n",
       "PEDROWNOTGRNT       object\n",
       "SPEEDING            object\n",
       "ST_COLCODE          object\n",
       "SEGLANEKEY           int64\n",
       "CROSSWALKKEY         int64\n",
       "HITPARKEDCAR        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check data formats\n",
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, some columns are not of the correct data type: \n",
    "* The multiclass categorical data of 'SDOT_COLCODE' needs to have type 'str' (object).\n",
    "* The Boolean values 'INATTENTIONIND', 'UNDERINFL', 'PEDROWNOTGRNT', 'SPEEDING', 'HITPARKEDCAR' need to be changed to numeric values 0/1 instead of True/False and therefore be of type 'float' or 'int' to make them more usable for the machine learning section later on. \n",
    "* Also, 'INCDTTM' needs to be converted to the datatype 'datetime'.\n",
    "\n",
    "While having a closer look at those attributes, let's also make sure to clean them of any duplicate or missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0    92089\n",
       "14.0    59300\n",
       "0.0     19158\n",
       "16.0    10935\n",
       "28.0     9599\n",
       "24.0     7694\n",
       "13.0     6887\n",
       "26.0     5072\n",
       "18.0     3434\n",
       "15.0     1820\n",
       "12.0     1781\n",
       "51.0     1472\n",
       "29.0      559\n",
       "21.0      207\n",
       "56.0      199\n",
       "27.0      173\n",
       "54.0      153\n",
       "23.0      129\n",
       "48.0      119\n",
       "31.0      111\n",
       "25.0      110\n",
       "34.0      100\n",
       "64.0       98\n",
       "69.0       86\n",
       "33.0       57\n",
       "55.0       56\n",
       "66.0       31\n",
       "22.0       19\n",
       "44.0       16\n",
       "32.0       13\n",
       "53.0        9\n",
       "61.0        7\n",
       "68.0        6\n",
       "35.0        6\n",
       "58.0        6\n",
       "46.0        4\n",
       "36.0        4\n",
       "47.0        2\n",
       "52.0        2\n",
       "87.0        1\n",
       "nan         1\n",
       "Name: SDOT_COLCODE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert SDOT_COLCODE to type 'str'\n",
    "df_new[['SDOT_COLCODE']] = df_new[['SDOT_COLCODE']].astype('str')\n",
    "\n",
    "#Check for NaN values\n",
    "df_new['SDOT_COLCODE'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDOT_COLCODE has one NaN value that we can drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of index with NaN values for SDOT_COLCODE attribute\n",
    "index_SDOT=df_new.loc[pd.isna(df_new[\"SDOT_COLCODE\"]), :].index\n",
    "\n",
    "#drop the rows with NaN values for ST_COLCODE attributes \n",
    "for i in index_SDOT:\n",
    "    df_new.drop(index_SDOT, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean attributes that have to be converted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    191337\n",
       "Y       30188\n",
       "Name: INATTENTIONIND, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for NaN values or duplicate values\n",
    "df_new['INATTENTIONIND'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N      103927\n",
       "0       81676\n",
       "NaN     26293\n",
       "Y        5399\n",
       "1        4230\n",
       "Name: UNDERINFL, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for NaN values or duplicate values\n",
    "df_new['UNDERINFL'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    216330\n",
       "Y        5195\n",
       "Name: PEDROWNOTGRNT, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for NaN values or duplicate values\n",
    "df_new['PEDROWNOTGRNT'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    211596\n",
       "Y        9929\n",
       "Name: SPEEDING, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for NaN values or duplicate values\n",
    "df_new['SPEEDING'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    209492\n",
       "Y     12033\n",
       "Name: HITPARKEDCAR, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for NaN values or duplicate values\n",
    "df_new['HITPARKEDCAR'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the attributes INATTENTIONIND, SPEEDING and PEDROWNOTGRNT people only entered 'Y' or did not enter anything at all, instead of entering 'N'. For those three attributes we will replace the 'Y' values with 1 and the NaN values with 0 and turn the dtype into float, to make them usable for the machine learning model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert True/False values to 1/0\n",
    "df_new[['INATTENTIONIND', 'PEDROWNOTGRNT', 'SPEEDING']] = df_new[['INATTENTIONIND', 'PEDROWNOTGRNT', 'SPEEDING']].replace({'Y': 1})\n",
    "df_new[['HITPARKEDCAR']] = df_new[['HITPARKEDCAR']].replace({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill NaN values with 0\n",
    "df_new[['INATTENTIONIND', 'PEDROWNOTGRNT', 'SPEEDING']] = df_new[['INATTENTIONIND', 'PEDROWNOTGRNT', 'SPEEDING']].fillna(0)\n",
    "\n",
    "#Convert boolean values to type 'float64'\n",
    "df_new[['INATTENTIONIND', 'PEDROWNOTGRNT', 'SPEEDING']] = df_new[['INATTENTIONIND', 'PEDROWNOTGRNT', 'SPEEDING']].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the attribute 'UNDERINFL' there is a mix between numeric and string boolean values, as well as some missing values. We will convert the 'Y' values into 1 and the 'N' values into 0 before having a closer look at the NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert True/False values to 1/0\n",
    "df_new[['UNDERINFL']] = df_new[['UNDERINFL']].replace({'Y': 1, 'N':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that, if we look at the other boolean values that indicate a contributing factor to the accident, the number of TRUE(1) values is roughly between 2-15% and we already have 5% TRUE values for the 'UNDERINFL' attribute, we will count the NaN values as FALSE(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['UNDERINFL']] = df_new[['UNDERINFL']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert boolean value to type 'float64'\n",
    "df_new[['UNDERINFL']] = df_new[['UNDERINFL']].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's change the attribute 'INCDTTM' to a datetime type and check for any possible missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert date and time values to type 'datetime64'\n",
    "df_new[['INCDTTM']]=df_new[['INCDTTM']].astype('datetime64')\n",
    "\n",
    "#Check for NaN values\n",
    "df_new['INCDTTM'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split date and time attribute**\n",
    "\n",
    "Time attributes might be very relevant when trying to predict accident severity. For example, there might be larger severity accidents during rush hour when people are commuting than during general working hours. That is why it makes sense to split the Datetime attribute into smaller increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, day, hour and weekday\n",
    "df_new['YEAR']=df_new['INCDTTM'].dt.year\n",
    "df_new['MONTH']=df_new['INCDTTM'].dt.strftime('%b')\n",
    "df_new['DAY']=df_new['INCDTTM'].dt.day\n",
    "df_new['HOUR']=df_new['INCDTTM'].dt.hour\n",
    "df_new['WEEKDAY']=df_new['INCDTTM'].dt.strftime('%a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X                         float64\n",
       "Y                         float64\n",
       "ADDRTYPE                   object\n",
       "INTKEY                    float64\n",
       "LOCATION                   object\n",
       "SEVERITYCODE               object\n",
       "SEVERITYDESC               object\n",
       "COLLISIONTYPE              object\n",
       "PERSONCOUNT                 int64\n",
       "PEDCOUNT                    int64\n",
       "PEDCYLCOUNT                 int64\n",
       "VEHCOUNT                    int64\n",
       "INJURIES                    int64\n",
       "SERIOUSINJURIES             int64\n",
       "FATALITIES                  int64\n",
       "INCDTTM            datetime64[ns]\n",
       "JUNCTIONTYPE               object\n",
       "SDOT_COLCODE               object\n",
       "INATTENTIONIND            float64\n",
       "UNDERINFL                 float64\n",
       "WEATHER                    object\n",
       "ROADCOND                   object\n",
       "LIGHTCOND                  object\n",
       "PEDROWNOTGRNT             float64\n",
       "SPEEDING                  float64\n",
       "ST_COLCODE                 object\n",
       "SEGLANEKEY                  int64\n",
       "CROSSWALKKEY                int64\n",
       "HITPARKEDCAR                int64\n",
       "YEAR                        int64\n",
       "MONTH                      object\n",
       "DAY                         int64\n",
       "HOUR                        int64\n",
       "WEEKDAY                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if the new attributes are in the correct format\n",
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we split the relevant time data into different attributes we can drop the original time attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df_new.drop(columns=['INCDTTM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean remaining categorical data**\n",
    "\n",
    "Before we continue with identifying and handling missing values, let's have a look at the categorical data we have not looked at yet to see if it needs some additional grouping and cleaning.\n",
    "\n",
    "We have ten categorical data attributes (excluding boolean) and one categorical label we need to inspect:\n",
    "\n",
    "<ul>\n",
    "   <li>1. \"ADDRTYPE\"</li>\n",
    "   <li>2. \"JUNCTIONTYPE\"</li>\n",
    "   <li>3. \"ST_COLCODE\"</li>    \n",
    "   <li>4. \"COLLISIONTYPE\"</li>\n",
    "   <li>5. \"WEATHER\"</li>\n",
    "   <li>6. \"ROADCOND\"</li>\n",
    "   <li>7. \"LIGHTCOND\"</li>\n",
    "   <li>8. \"SEGLANEKEY\"</li>\n",
    "   <li>9. \"CROSSWALKKEY\"</li>\n",
    "   <li>10. \"SEVERITYDESC\"</li>\n",
    "   <li>11. Label:\"SEVERITYCODE\"</li>\n",
    "</ul>\n",
    "\n",
    "Let's go through each of them to see if they need to be cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 ADDRTYPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block           144999\n",
       "Intersection     71936\n",
       "NaN               3712\n",
       "Alley              878\n",
       "Name: ADDRTYPE, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['ADDRTYPE'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually tell if a ADDRTYPE is a Block, Intersection or Alley by looking at the LOCATION attribute, as they are very related:\n",
    "\n",
    "* For Blocks the phrasing of the LOCATION is: \"X BETWEEN Y AND Z\"\n",
    "* For Intersections the phrasing of LOCATION is: \"X AND Y\"\n",
    "* For Alleys the LOCATION always has a NaN value, while LOCATION never has a NaN value for ADDRTYPE Block or Intersection.\n",
    "\n",
    "\n",
    "If we look at the rows with ADDRTYPE NaN, all of them also have LOCATION NaN. Because LOCATION NaN always mean Alley we will assign them as Alleys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign missing values the value 'Alley'\n",
    "df_new[['ADDRTYPE']] = df_new[['ADDRTYPE']].fillna('Alley')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 JUNCTIONTYPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mid-Block (not related to intersection)              101709\n",
       "At Intersection (intersection related)                69228\n",
       "Mid-Block (but intersection related)                  24410\n",
       "NaN                                                   11974\n",
       "Driveway Junction                                     11496\n",
       "At Intersection (but not related to intersection)      2497\n",
       "Ramp Junction                                           190\n",
       "Unknown                                                  21\n",
       "Name: JUNCTIONTYPE, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['JUNCTIONTYPE'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUNCTIONTYPE has two different types of missing values: Unknown and NaN. We will replace Unknown with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"unknown\" to NaN\n",
    "df_new['JUNCTIONTYPE'].replace(\"Unknown\", np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46% of all accidents happen Mid-Block (not related to intersection). That is why we replace missing values with 'Mid-Block (not related to intersection)' as the most frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing values with most frequent value\n",
    "df_new['JUNCTIONTYPE'] = df_new['JUNCTIONTYPE'].fillna('Mid-Block (not related to intersection)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 ST_COLCODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9413"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['ST_COLCODE'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing vales for ST_COLCODE cannot realistically be replaced with other data. We will therefore drop the rows missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list=[]\n",
    "\n",
    "#get list of index with NaN values for ST_COLCODE attribute\n",
    "index_list=df_new.loc[pd.isna(df_new[\"ST_COLCODE\"]), :].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    22,     42,     61,    115,    141,    155,    167,    223,\n",
       "               229,    352,\n",
       "            ...\n",
       "            195111, 195398, 195633, 196569, 196576, 196693, 196745, 196970,\n",
       "            197205, 197521],\n",
       "           dtype='int64', length=9413)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with NaN values for ST_COLCODE attributes \n",
    "\n",
    "df_new.drop(df.index[index_list], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 COLLISIONTYPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parked Car    48551\n",
       "Angles        35573\n",
       "Rear Ended    34691\n",
       "Other         24588\n",
       "Sideswipe     18891\n",
       "NaN           16900\n",
       "Left Turn     14115\n",
       "Pedestrian     7666\n",
       "Cycles         5932\n",
       "Right Turn     3017\n",
       "Head On        2188\n",
       "Name: COLLISIONTYPE, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['COLLISIONTYPE'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive the missing COLLISIONTYPE values by taking them from the descriptions of ST_COLCODE for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NaN values for COLLISIONTYPE by taking them from the descriptions of ST_COLCODE for each row and matching those values to categories fro COLLISIONTYPE.\n",
    "\n",
    "index_coll=df_new.loc[pd.isna(df_new['COLLISIONTYPE']), :].index\n",
    "\n",
    "pedestrian=['0','1','2','3','4','5']\n",
    "angles=['10']\n",
    "sideswipe=['11','12','26','27','71','72','81','82']\n",
    "rear_ended=['13','14','83','84','73','74']\n",
    "left_turn=['15','28','29']\n",
    "right_turn=['16']\n",
    "parked_car=['19','20','32']\n",
    "other=['21','22','23','31','40','41','42','43','47','48','49','50','51','52','53','54','55','56','57','58','59','60','61','62','63','64','65','66','67']\n",
    "head_on=['24','25','30']\n",
    "cycles=['44','45','46']\n",
    "\n",
    "for i in index_coll:\n",
    "    if df_new.loc[i, 'ST_COLCODE'] in pedestrian:\n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Pedestrian'\n",
    "    elif df_new.loc[i, 'ST_COLCODE'] in angles:\n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Angles'\n",
    "    elif df_new.loc[i, 'ST_COLCODE'] in sideswipe:\n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Sideswipe'\n",
    "    elif df_new.loc[i, 'ST_COLCODE'] in rear_ended:\n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Rear Ended'\n",
    "    elif df_new.loc[i, 'ST_COLCODE'] in left_turn:\n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Left Turn'\n",
    "    elif df_new.loc[i, 'ST_COLCODE'] in right_turn:\n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Right Turn'\n",
    "    elif df_new.loc[i, 'ST_COLCODE'] in parked_car:\n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Parked Car'\n",
    "    elif df_new.loc[i, 'ST_COLCODE'] in other:\n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Other'    \n",
    "    elif df_new.loc[i, 'ST_COLCODE'] in head_on:\n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Head On'\n",
    "    else: \n",
    "        df_new.loc[i,'COLLISIONTYPE'] = 'Cycles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 WEATHER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['WEATHER'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEATHER has two different types of missing values: Unknown and NaN. We will replace Unknown with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"unknown\" to NaN\n",
    "df_new['WEATHER'].replace(\"Unknown\", np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 60% of all accidents happen in clear weather. That is why we replace missing values with 'Clear' as the most frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing values with most frequent value\n",
    "df_new['WEATHER'] = df_new['WEATHER'].fillna('Clear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 ROADCOND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['ROADCOND'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROADCOND also has two different types of missing values: Unknown and NaN. We will replace Unknown with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"unknown\" to NaN\n",
    "df_new['ROADCOND'].replace(\"Unknown\", np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 60% of all accidents happen on dry road. That is why we replace missing values with 'Dry' as the most frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing values with most frequent value\n",
    "df_new['ROADCOND'] = df_new['ROADCOND'].fillna('Dry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7 LIGHTCOND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['LIGHTCOND'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIGHTCOND also has two different types of missing values: Unknown and NaN. We will replace Unknown with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"unknown\" to NaN\n",
    "df_new['LIGHTCOND'].replace(\"Unknown\", np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive the missing values for LIGHTCOND with a combination of MONTH and HOUR data.  We can at least tell wheather it was dark or light outside for Seattle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_data = {'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], 'Day(Hour)': [8,7,6,6,5,5,4,5,6,6,6,7], 'Night(Hour)': [17,17,18,20,21,21,22,21,20,19,17,17]}\n",
    "seattle_light = pd.DataFrame(data=light_data)\n",
    "seattle_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NaN values for LIGHTCOND with either Daylight or Dark - Unknown Lighting\n",
    "\n",
    "index_light=df_new.loc[pd.isna(df_new['LIGHTCOND']), :].index\n",
    "\n",
    "for i in index_light:\n",
    "    if (df_new.loc[i,'MONTH'] == 'Jan') & (df_new.loc[i,'HOUR'] >=8) & (df_new.loc[i,'HOUR'] <=17) or (df_new.loc[i,'MONTH'] == 'Feb') & (df_new.loc[i,'HOUR'] >=7) & (df_new.loc[i,'HOUR'] <=17) or (df_new.loc[i,'MONTH'] == 'Mar') & (df_new.loc[i,'HOUR'] >=6) & (df_new.loc[i,'HOUR'] <=18) or (df_new.loc[i,'MONTH'] == 'Apr') & (df_new.loc[i,'HOUR'] >=6) & (df_new.loc[i,'HOUR'] <=20) or (df_new.loc[i,'MONTH'] == 'May') & (df_new.loc[i,'HOUR'] >=5) & (df_new.loc[i,'HOUR'] <=21) or (df_new.loc[i,'MONTH'] == 'Jun') & (df_new.loc[i,'HOUR'] >=5) & (df_new.loc[i,'HOUR'] <=21) or (df_new.loc[i,'MONTH'] == 'Jul') & (df_new.loc[i,'HOUR'] >=4) & (df_new.loc[i,'HOUR'] <=22) or (df_new.loc[i,'MONTH'] == 'Aug') & (df_new.loc[i,'HOUR'] >=5) & (df_new.loc[i,'HOUR'] <=21) or (df_new.loc[i,'MONTH'] == 'Sep') & (df_new.loc[i,'HOUR'] >=6) & (df_new.loc[i,'HOUR'] <=20) or (df_new.loc[i,'MONTH'] == 'Oct') & (df_new.loc[i,'HOUR'] >=6) & (df_new.loc[i,'HOUR'] <=19) or (df_new.loc[i,'MONTH'] == 'Nov') & (df_new.loc[i,'HOUR'] >=6) & (df_new.loc[i,'HOUR'] <=17) or (df_new.loc[i,'MONTH'] == 'Dec') & (df_new.loc[i,'HOUR'] >=7) & (df_new.loc[i,'HOUR'] <=17) :\n",
    "        df_new.loc[i,'LIGHTCOND'] = 'Daylight'\n",
    "    else:\n",
    "        df_new.loc[i,'LIGHTCOND'] = 'Dark - Unknown Lighting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8 SEGLANEKEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['SEGLANEKEY'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEGLANEKEY is mainly consists of 0 as a key, which is most likely a missing value, that cannot be replaced because of the high number of missing values. We should therefore drop SEGLANEKEY entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column\n",
    "df_new=df_new.drop(columns=['SEGLANEKEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9 CROSSWALKKEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['CROSSWALKKEY'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROSSWALKKEY is mainly consists of 0 as a key, which is most likely a missing value, that cannot be replaced because of the high number of missing values. We should therefore drop CROSSWALKKEY entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column\n",
    "df_new=df_new.drop(columns=['CROSSWALKKEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 SEVERITYDESC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['SEVERITYDESC'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEVERITYDESC has one type of missing value: Unknown. First we need to change Unknown to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"unknown\" to NaN\n",
    "df_new['SEVERITYDESC'].replace(\"Unknown\", np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now assign the correct SEVERITYDESC to the rows missing data by combining the SEVERITYDESC with INJURIES, SERIOUSINJURIES and FATALITIES. But first, let's have a look at INJURIES, SERIOUSINJURIES and FATALITIES to see if they need cleaning ahead of us replacing the missing values of SEVERITYCODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are there cases where they are not mutually exclusive?\n",
    "\n",
    "mut_ex=df_new[(df_new['INJURIES'] > 0) & (df_new['SERIOUSINJURIES'] > 0) & (df_new['FATALITIES'] > 0)]\n",
    "mut_ex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that INJURIES, SERIOUSINJURIES and FATALITIES are not mutually exclusive. This might pose a problem when we want to use those attributes to create the missing values for SEVERITYDESC.\n",
    "\n",
    "We can solve this by combining multiple conditions for the values of INJURIES, SERIOUSINJURIES and FATALITIES, that allows us to only consider the maximum mentioned damage for SEVERITYDESC, ranking property damage, injury, serious injury and fatality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NaN values for SEVERITYDESC\n",
    "\n",
    "index_sevdesc=df_new.loc[pd.isna(df_new['SEVERITYDESC']), :].index\n",
    "\n",
    "for i in index_sevdesc:\n",
    "    if (df_new.loc[i, 'INJURIES']>0) and not (((df_new.loc[i, 'SERIOUSINJURIES']>0) and (df_new.loc[i, 'FATALITIES']>0)) or (df_new.loc[i, 'SERIOUSINJURIES']>0) or (df_new.loc[i, 'FATALITIES']>0)):\n",
    "        df_new.loc[i,'SEVERITYDESC'] = 'Injury Collision'\n",
    "    elif (df_new.loc[i, 'SERIOUSINJURIES']>0) and not (df_new.loc[i, 'FATALITIES']>0):\n",
    "        df_new.loc[i,'SEVERITYDESC'] = 'Serious Injury Collision'\n",
    "    elif df_new.loc[i, 'FATALITIES']>0:\n",
    "        df_new.loc[i,'SEVERITYDESC'] = 'Fatality Collision'\n",
    "    else: \n",
    "        df_new.loc[i,'SEVERITYDESC'] = 'Property Damage Only Collision'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11 Label: SEVERITYCODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for missing or duplicate values\n",
    "df_new['SEVERITYCODE'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEVERITYCODE also has two different types of missing values: 0(Unknown) and NaN. First, we will replace 0(Unknown) with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"0\" to NaN\n",
    "df_new['SEVERITYCODE'].replace(\"0\", np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign the correct SEVERITYCODE to the rows missing data by combining the SEVERITYCODE with SEVERITYDESC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NaN values for SEVERITYCODE\n",
    "\n",
    "index_sev=df_new.loc[pd.isna(df_new['SEVERITYCODE']), :].index\n",
    "\n",
    "for i in index_sev:\n",
    "    if df_new.loc[i, 'SEVERITYDESC']=='Injury Collision':\n",
    "        df_new.loc[i,'SEVERITYCODE'] = '2'\n",
    "    elif df_new.loc[i, 'SEVERITYDESC']=='Serious Injury Collision':\n",
    "        df_new.loc[i,'SEVERITYCODE'] = '2b'\n",
    "    elif df_new.loc[i, 'SEVERITYDESC']=='Fatality Collision':\n",
    "        df_new.loc[i,'SEVERITYCODE'] = '3'\n",
    "    else: \n",
    "        df_new.loc[i,'SEVERITYCODE'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better output during the data exploration phase, let's also change the values of SEVERITYCODE to a numerical sequence from 0-3 rather than 1, 2, 2b, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 1, 2, 2b, 3 values to 0, 1, 2, 3\n",
    "df_new[['SEVERITYCODE']] = df_new[['SEVERITYCODE']].replace({'1': '0', '2': '1', '2b': '2', '3': '3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's change SEVERITYCODE from object to integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert SEVERITYCODE values to type 'int64'\n",
    "df_new[['SEVERITYCODE']]=df_new[['SEVERITYCODE']].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify and handle remaining missing values**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Identify missing values\n",
    "missing_data = df_new.isnull()\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a for loop, we can figure out the number of missing values in each column. \"True\" represents a missing value, \"False\" means the value is present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count missing values in each column\n",
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data[column].value_counts())\n",
    "    print(\"\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary above, each column has 212,112 rows of data, with 4 columns containing missing data:\n",
    "<ol>\n",
    "    <li>\"X\": 6664 missing data points</li>\n",
    "    <li>\"Y\": 6664 missing data points</li>\n",
    "    <li>\"INTKEY\": 142758 missing data points</li>\n",
    "    <li>\"LOCATION\": 3910 missing data points</li>  \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle missing values**\n",
    "\n",
    "Whole columns should be dropped only if most entries in the column are empty.\n",
    "In our dataset, INTKEY has enough missing values to drop entirely (~67%).\n",
    "We can also drop whole columns for X, Y and Location. They have a lot of missing values we cannot realistically retrieve from other sources and aren't really necessary to predict severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "\n",
    "df_new=df_new.drop(columns=['X', 'Y', 'LOCATION', 'INTKEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one final check to see if we got rid of all missing data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_check = df_new.isnull()\n",
    "missing_data_check.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in missing_data_check.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data_check[column].value_counts())\n",
    "    print(\"\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All NaN values now have been removed and we still have 212,112 individual accident events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature selection\n",
    "\n",
    "After data cleaning, there were 212,112 samples and 26 attributes in the data. Upon examining each attribute, it was clear that there was some further redundancy. \n",
    "\n",
    "ST_COLCODE indicates the type of collision and is therefore almost identical with COLLISIONTYPE, so I decided to drop ST_COLCODE.\n",
    "\n",
    "SEVERITYDESC is the more detailed description version of SEVERITYCODE. I therefore decided to drop SEVERITYDESC to decrease redundancy.\n",
    "\n",
    "After all, 24 features were selected for data exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop SEVERITYDESC and ST_COLCODE\n",
    "\n",
    "df_cleaned=df_new.drop(columns=['SEVERITYDESC', 'ST_COLCODE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology <a name=\"methodology\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will try to predict the severity of an accident in the Seattle area based on different predictor features.\n",
    "\n",
    "In our first step we have collected a suitable database that includes accident severity as a label and a number of additional attributes that could serve as predictor features potentially. \n",
    "\n",
    "Our second step was to cleaned and wrangled the dataset: \n",
    "<li>Obviously irrelevant features have been dismissed</li>\n",
    "<li>Categorical data has been cleaned</li>\n",
    "<li>Features containing any missing data points have been cleaned and dropped if necessary</li>\n",
    "\n",
    "The next step in our analysis will be the exploration of the relationship between SEVERITYCODE and our potential predictor features to get a better understanding of possible good candidates for features in our ML models. After finishing the data exploration we will drop features which do not serve as good predictor attributes for SEVERITYCODE.\n",
    "\n",
    "We will then go ahead and preprocess the remaining data for our ML models by turning categorical data into numerical data and splitting our data set into training and testing data.\n",
    "\n",
    "Lastly, we will try ML different models and decide which one is the most suitable for the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Explorative analysis\n",
    "**Relationship between Severity and the numerical attributes**\n",
    "\n",
    "In total, there are 7 numerical attributes where we need to explore the relationship to SEVERITYCODE further:\n",
    "* 1 PERSONCOUNT\n",
    "* 2 PEDCOUNT\n",
    "* 3 PEDCYLCOUNT\n",
    "* 4 VEHCOUNT\n",
    "* 5 INJURIES\n",
    "* 6 SERIOUSINJURIES\n",
    "* 7 FATALITIES\n",
    "\n",
    "For the numerical attributes in our data set I decided to use a scatterplot visualization and the Kendall’s rank coefficient combined with the p value for quantitative analysis.\n",
    "\n",
    "The Kendall’s rank coefficient allows the comparison of columns of ranked data. A value close to 0 means no relationship and values close to 1 mean a perfect relationship. The test can also produce negative values, but they can just be treated like positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 Relationship between SEVERITYCODE and PERSONCOUNT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "pc=sns.regplot(x=\"PERSONCOUNT\", y=\"SEVERITYCODE\", data=df_cleaned, x_jitter=.4, y_jitter=.4)\n",
    "pc.set(ylim=(None,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression line is almost horizontal and the data points are very far from the fitted line. Therefore it might not be a reliable variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tau, p_value = stats.kendalltau(df_cleaned[\"PERSONCOUNT\"], df_cleaned[\"SEVERITYCODE\"])\n",
    "print(tau)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "<p>Since the p-value is $<$ 0.001, the correlation between PERSONCOUNT and SEVERITYCODE is statistically significant, although the linear relationship is weak (~0.176)</p>\n",
    "\n",
    "***=> PERSONCOUNT is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 Relationship between SEVERITYCODE and PEDCOUNT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "pc=sns.regplot(x=\"PEDCOUNT\", y=\"SEVERITYCODE\", data=df_cleaned, x_jitter=.4, y_jitter=.4)\n",
    "pc.set(ylim=(None,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Even though the regression line is almost a perfect diagonal, the data points are very far from the fitted line. Therefore PERSONCOUNT does not seem to be a reliable variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tau, p_value = stats.kendalltau(df_cleaned[\"PEDCOUNT\"], df_cleaned[\"SEVERITYCODE\"])\n",
    "print(tau)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "<p>Since the p-value is $<$ 0.001, the correlation between PEDCOUNT and SEVERITYCODE is statistically significant, although the linear relationship is weak (~0.278)</p>\n",
    "\n",
    "***=> PEDCOUNT is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 Relationship between SEVERITYCODE and PEDCYLCOUNT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "pcc=sns.regplot(x=\"PEDCYLCOUNT\", y=\"SEVERITYCODE\", data=df_cleaned, x_jitter=.4, y_jitter=.4)\n",
    "pcc.set(ylim=(None,4), xticks=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The regression line isn't very steep and the data points are very far from the fitted line. Therefore it might not be a reliable variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tau, p_value = stats.kendalltau(df_cleaned[\"PEDCYLCOUNT\"], df_cleaned[\"SEVERITYCODE\"])\n",
    "print(tau)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "<p>Since the p-value is $<$ 0.001, the correlation between PEDCYLCOUNT and SEVERITYCODE is statistically significant, although the linear relationship is weak (~0.222)</p>\n",
    "\n",
    "***=> PEDCYLCOUNT is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 Relationship between SEVERITYCODE and VEHCOUNT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "pcc=sns.regplot(x=\"VEHCOUNT\", y=\"SEVERITYCODE\", data=df_cleaned, x_jitter=.4, y_jitter=.4)\n",
    "pcc.set(ylim=(None,4),  xticks=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression line is almost horizontal and the data points are very far from the fitted line. Therefore it might not be a reliable variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tau, p_value = stats.kendalltau(df_cleaned[\"VEHCOUNT\"], df_cleaned[\"SEVERITYCODE\"])\n",
    "print(tau)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "<p>Since the p-value is $>$ 0.001, the correlation between VEHCOUNT and SEVERITYCODE is statistically significant and the linear relationship is weak (~0.021)</p>\n",
    "\n",
    "***=> VEHCOUNT is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 Relationship between SEVERITYCODE and INJURIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "pcc=sns.regplot(x=\"INJURIES\", y=\"SEVERITYCODE\", data=df_cleaned, x_jitter=.4, y_jitter=.4)\n",
    "pcc.set(ylim=(0,4), xlim=(None,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The regression line is almost a perfect diagonal and the data points are fairly close to the fitted line. Therefore INJURIES does seem to be a reliable variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tau, p_value = stats.kendalltau(df_cleaned[\"INJURIES\"], df_cleaned[\"SEVERITYCODE\"])\n",
    "print(tau)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "<p>Since the p-value is $<$ 0.001, the correlation between INJURIES and SEVERITYCODE is statistically significant and the linear relationship is strong (~0.949)</p>\n",
    "\n",
    "***=> INJURIES is a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 Relationship between SEVERITYCODE and SERIOUSINJURIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "pcc=sns.regplot(x=\"SERIOUSINJURIES\", y=\"SEVERITYCODE\", data=df_cleaned, x_jitter=.4, y_jitter=.4)\n",
    "pcc.set(ylim=(0,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Even though the regression line is almost a perfect diagonal, the data points are very far from the fitted line. Therefore SERIOUSINJURIES does not seem to be a reliable variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tau, p_value = stats.kendalltau(df_cleaned[\"SERIOUSINJURIES\"], df_cleaned[\"SEVERITYCODE\"])\n",
    "print(tau)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "<p>Since the p-value is $<$ 0.001, the correlation between SERIOUSINJURIES and SEVERITYCODE is statistically significant, but the linear relationship is weak (~0.260)</p>\n",
    "\n",
    "***=> SERIOUSINJURIES is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7 Relationship between SEVERITYCODE and FATALITIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "pcc=sns.regplot(x=\"FATALITIES\", y=\"SEVERITYCODE\", data=df_cleaned, x_jitter=.4, y_jitter=.4)\n",
    "pcc.set(ylim=(0,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Even though the regression line is almost a perfect diagonal, the data points are very far from the fitted line. Therefore FATALITIES does not seem to be a reliable variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, p_value = stats.kendalltau(df_cleaned[\"FATALITIES\"], df_cleaned[\"SEVERITYCODE\"])\n",
    "print(tau)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "<p>Since the p-value is $<$ 0.001, the correlation between FATALITIES and SEVERITYCODE is statistically significant, but the linear relationship is weak (~0.088)</p>\n",
    "\n",
    "***=> FATALITIES is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationship between Severity and the categorical attributes**\n",
    "\n",
    "A good way to visualize categorical variables and identify their relationship with the label variable is by using boxplots. We will visualize all categorical feature attributes and also look at the frequency of each categorical value of an attribute. This will give us a good idea whether or not an attribute would be a helpful predictor variable.\n",
    "\n",
    "\n",
    "In total, there are 17 categorical attributes for which we need to explore the relationship to SEVERITYCODE further:\n",
    "* 1 ADDRTYPE\n",
    "* 2 COLLISIONTYPE\n",
    "* 3 JUNCTIONTYPE\n",
    "* 4 SDOT_COLCODE\n",
    "* 5 INATTENTIONIND\n",
    "* 6 UNDERINFL\n",
    "* 7 WEATHER\n",
    "* 8 ROADCOND\n",
    "* 9 LIGHTCOND\n",
    "* 10 PEDROWNOTGRNT\n",
    "* 11 SPEEDING\n",
    "* 12 HITPARKEDCAR\n",
    "* 13 YEAR\n",
    "* 14 MONTH\n",
    "* 15 DAY\n",
    "* 16 HOUR\n",
    "* 17 WEEKDAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 Relationship between SEVERITYCODE and ADDRTYPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"ADDRTYPE\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDRTYPE_counts = df_cleaned['ADDRTYPE'].value_counts().to_frame()\n",
    "ADDRTYPE_counts.rename(columns={'ADDRTYPE': 'value_counts'}, inplace=True)\n",
    "ADDRTYPE_counts.index.name = 'ADDRTYPE'\n",
    "ADDRTYPE_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different ADDRTYPE categories differs and frequency distribution between the categories is acceptable.\n",
    "\n",
    "***=> ADDRTYPE is a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 Relationship between SEVERITYCODE and COLLISIONTYPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "sns.boxplot(x=\"COLLISIONTYPE\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLISIONTYPE_counts = df_cleaned['COLLISIONTYPE'].value_counts().to_frame()\n",
    "COLLISIONTYPE_counts.rename(columns={'COLLISIONTYPE': 'value_counts'}, inplace=True)\n",
    "COLLISIONTYPE_counts.index.name = 'COLLISIONTYPE'\n",
    "COLLISIONTYPE_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of COLLISIONTYPE have significant overlap, but they frequency distribution between the categories is not skewed.\n",
    "\n",
    "***=> Because of the overlap COLLISIONTYPE is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 Relationship between SEVERITYCODE and JUNCTIONTYPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,8))\n",
    "sns.boxplot(x=\"JUNCTIONTYPE\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUNCTIONTYPE_counts = df_cleaned['JUNCTIONTYPE'].value_counts().to_frame()\n",
    "JUNCTIONTYPE_counts.rename(columns={'JUNCTIONTYPE': 'value_counts'}, inplace=True)\n",
    "JUNCTIONTYPE_counts.index.name = 'JUNCTIONTYPE'\n",
    "JUNCTIONTYPE_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of JUNCTIONTYPE have significant overlap, but they frequency distribution between the categories is not skewed.\n",
    "\n",
    "***=> Because of the overlap JUNCTIONTYPE is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 Relationship between SEVERITYCODE and SDOT_COLCODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "sns.boxplot(x=\"SDOT_COLCODE\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDOT_COLCODE_counts = df_cleaned['SDOT_COLCODE'].value_counts().to_frame()\n",
    "SDOT_COLCODE_counts.rename(columns={'SDOT_COLCODE': 'value_counts'}, inplace=True)\n",
    "SDOT_COLCODE_counts.index.name = 'SDOT_COLCODE'\n",
    "SDOT_COLCODE_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We see that the distribution of SEVERITYCODE between the different SDOT_COLCODE categories differs; as such SDOT_COLCODE could potentially be a predictor of SEVERITYCODE. However, looking at the value counts for the individual categories, some of them have a little as 1 value count, so the results are skewed. Thus, we are not able to draw any conclusions about the SDOT_COLCODE.\n",
    "\n",
    "***=> SDOT_COLCODE is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 Relationship between SEVERITYCODE and INATTENTIONIND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"INATTENTIONIND\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INATTENTIONIND_counts = df_cleaned['INATTENTIONIND'].value_counts().to_frame()\n",
    "INATTENTIONIND_counts.rename(columns={'INATTENTIONIND': 'value_counts'}, inplace=True)\n",
    "INATTENTIONIND_counts.index.name = 'INATTENTIONIND'\n",
    "INATTENTIONIND_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of INATTENTIONIND have significant overlap, but they frequency distribution between the categories is not skewed.\n",
    "\n",
    "***=> Because of the overlap INATTENTIONIND is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 Relationship between SEVERITYCODE and UNDERINFL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"UNDERINFL\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDERINFL_counts = df_cleaned['UNDERINFL'].value_counts().to_frame()\n",
    "UNDERINFL_counts.rename(columns={'UNDERINFL': 'value_counts'}, inplace=True)\n",
    "UNDERINFL_counts.index.name = 'UNDERINFL'\n",
    "UNDERINFL_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of UNDERINFL have significant overlap, but they frequency distribution between the categories is not skewed.\n",
    "\n",
    "***=> Because of the overlap UNDERINFL is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7 Relationship between SEVERITYCODE and WEATHER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "sns.boxplot(x=\"WEATHER\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_counts = df_cleaned['WEATHER'].value_counts().to_frame()\n",
    "WEATHER_counts.rename(columns={'WEATHER': 'value_counts'}, inplace=True)\n",
    "WEATHER_counts.index.name = 'WEATHER'\n",
    "WEATHER_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We see that the distribution of SEVERITYCODE between the different WEATHER categories differs; as such WEATHER could potentially be a predictor of SEVERITYCODE. However, looking at the value counts for the individual categories, some of them have a little as 1 value count, so the results are skewed. Thus, we are not able to draw any conclusions about the WEATHER.\n",
    "\n",
    "***=> WEATHER is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8 Relationship between SEVERITYCODE and ROADCOND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "sns.boxplot(x=\"ROADCOND\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROADCOND_counts = df_cleaned['ROADCOND'].value_counts().to_frame()\n",
    "ROADCOND_counts.rename(columns={'ROADCOND': 'value_counts'}, inplace=True)\n",
    "ROADCOND_counts.index.name = 'ROADCOND'\n",
    "ROADCOND_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of ROADCOND have significant overlap, but they frequency distribution between the categories is not skewed.\n",
    "\n",
    "***=> Because of the overlap ROADCOND is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9 Relationship between SEVERITYCODE and LIGHTCOND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "sns.boxplot(x=\"LIGHTCOND\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIGHTCOND_counts = df_cleaned['LIGHTCOND'].value_counts().to_frame()\n",
    "LIGHTCOND_counts.rename(columns={'LIGHTCOND': 'value_counts'}, inplace=True)\n",
    "LIGHTCOND_counts.index.name = 'LIGHTCOND'\n",
    "LIGHTCOND_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of LIGHTCOND have significant overlap, but they frequency distribution between the categories is not skewed.\n",
    "\n",
    "***=> Because of the overlap LIGHTCOND is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 Relationship between SEVERITYCODE and PEDROWNOTGRNT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"PEDROWNOTGRNT\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEDROWNOTGRNT_counts = df_cleaned['PEDROWNOTGRNT'].value_counts().to_frame()\n",
    "PEDROWNOTGRNT_counts.rename(columns={'PEDROWNOTGRNT': 'value_counts'}, inplace=True)\n",
    "PEDROWNOTGRNT_counts.index.name = 'PEDROWNOTGRNT'\n",
    "PEDROWNOTGRNT_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different PEDROWNOTGRNT categories differs and frequency distribution between the categories is acceptable.\n",
    "\n",
    "***=> PEDROWNOTGRNT is a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11 Relationship between SEVERITYCODE and SPEEDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"SPEEDING\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEEDING_counts = df_cleaned['SPEEDING'].value_counts().to_frame()\n",
    "SPEEDING_counts.rename(columns={'SPEEDING': 'value_counts'}, inplace=True)\n",
    "SPEEDING_counts.index.name = 'SPEEDING'\n",
    "SPEEDING_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of SPEEDING have significant overlap, but they frequency distribution between the categories is not skewed.\n",
    "\n",
    "***=> Because of the overlap SPEEDING is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12 Relationship between SEVERITYCODE and HITPARKEDCAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"HITPARKEDCAR\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HITPARKEDCAR_counts = df_cleaned['HITPARKEDCAR'].value_counts().to_frame()\n",
    "HITPARKEDCAR_counts.rename(columns={'HITPARKEDCAR': 'value_counts'}, inplace=True)\n",
    "HITPARKEDCAR_counts.index.name = 'HITPARKEDCAR'\n",
    "HITPARKEDCAR_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different HITPARKEDCAR categories differs and frequency distribution between the categories is acceptable.\n",
    "\n",
    "***=> HITPARKEDCAR is a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13 Relationship between SEVERITYCODE and YEAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "sns.boxplot(x=\"YEAR\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_counts = df_cleaned['YEAR'].value_counts().to_frame()\n",
    "YEAR_counts.rename(columns={'YEAR': 'value_counts'}, inplace=True)\n",
    "YEAR_counts.index.name = 'YEAR'\n",
    "YEAR_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of YEAR have significant overlap, but they frequency distribution between the categories is not overly skewed.\n",
    "\n",
    "***=> Because of the overlap YEAR is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14 Relationship between SEVERITYCODE and MONTH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "sns.boxplot(x=\"MONTH\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH_counts = df_cleaned['MONTH'].value_counts().to_frame()\n",
    "MONTH_counts.rename(columns={'MONTH': 'value_counts'}, inplace=True)\n",
    "MONTH_counts.index.name = 'MONTH'\n",
    "MONTH_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of MONTH have significant overlap, but they frequency distribution between the categories is not overly skewed.\n",
    "\n",
    "***=> Because of the overlap MONTH is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15 Relationship between SEVERITYCODE and DAY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "sns.boxplot(x=\"DAY\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY_counts = df_cleaned['DAY'].value_counts().to_frame()\n",
    "DAY_counts.rename(columns={'DAY': 'value_counts'}, inplace=True)\n",
    "DAY_counts.index.name = 'DAY'\n",
    "DAY_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of DAY have significant overlap, but they frequency distribution between the categories is not overly skewed.\n",
    "\n",
    "***=> Because of the overlap DAY is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16 Relationship between SEVERITYCODE and HOUR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"HOUR\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUR_counts = df_cleaned['HOUR'].value_counts().to_frame()\n",
    "HOUR_counts.rename(columns={'HOUR': 'value_counts'}, inplace=True)\n",
    "HOUR_counts.index.name = 'HOUR'\n",
    "HOUR_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of HOUR have significant overlap, but they frequency distribution between the categories is not overly skewed.\n",
    "\n",
    "***=> Because of the overlap HOUR is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17 Relationship between SEVERITYCODE and WEEKDAY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"WEEKDAY\", y=\"SEVERITYCODE\", data=df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEKDAY_counts = df_cleaned['WEEKDAY'].value_counts().to_frame()\n",
    "WEEKDAY_counts.rename(columns={'WEEKDAY': 'value_counts'}, inplace=True)\n",
    "WEEKDAY_counts.index.name = 'WEEKDAY'\n",
    "WEEKDAY_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "The distribution of SEVERITYCODE between the different categories of WEEKDAY have significant overlap, but they frequency distribution between the categories is not skewed.\n",
    "\n",
    "***=> Because of the overlap WEEKDAY is not a good predictor of SEVERITYCODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusion: Important Attributes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We now have a better idea of what our data looks like and which variables are important to take into account when predicting the SEVERITYCODE of a collision. We have narrowed it down to the following variables:</p>\n",
    "\n",
    "Continuous numerical variables:\n",
    "<ul>\n",
    "    <li>INJURIES</li>\n",
    "</ul>\n",
    "    \n",
    "Categorical variables:\n",
    "<ul>\n",
    "    <li>ADDRTYPE</li>\n",
    "    <li>PEDROWNOTGRNT</li>\n",
    "    <li>HITPARKEDCAR</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means we can **drop the following attributes** from our dataset:</p>\n",
    "\n",
    "Continuous numerical variables:\n",
    "<ul>\n",
    "    <li>PERSONCOUNT</li>\n",
    "    <li>PEDCOUNT</li>\n",
    "    <li>PEDCYLCOUNT</li>\n",
    "    <li>VEHCOUNT</li>\n",
    "    <li>SERIOUSINJURIES</li>\n",
    "    <li>FATALITIES</li>\n",
    "</ul>\n",
    "    \n",
    "Categorical variables:\n",
    "<ul>\n",
    "    <li>COLLISIONTYPE</li>\n",
    "    <li>JUNCTIONTYPE</li>\n",
    "    <li>SDOT_COLCODE </li>\n",
    "    <li>INATTENTIONIND</li>    \n",
    "    <li>UNDERINFL</li>    \n",
    "    <li>WEATHER</li>\n",
    "    <li>ROADCOND</li>\n",
    "    <li>LIGHTCOND</li>\n",
    "    <li>SPEEDING</li>\n",
    "    <li>YEAR</li>\n",
    "    <li>MONTH</li>\n",
    "    <li>DAY</li>\n",
    "    <li>HOUR</li>\n",
    "    <li>WEEKDAY</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "\n",
    "df_explored=df_cleaned.drop(columns=['PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT', 'SERIOUSINJURIES', 'FATALITIES', 'COLLISIONTYPE', 'JUNCTIONTYPE', 'INATTENTIONIND', 'UNDERINFL', 'WEATHER', 'ROADCOND', 'LIGHTCOND', 'SPEEDING', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'WEEKDAY', 'SDOT_COLCODE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data preparation for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_dummies()**\n",
    "\n",
    "Most ML models do not understand string values, only numbers. That is why we have to translate categorical values into numbers. We do so by using the get_dummies() method. This method assigns a numerical variable to a category of an attribute. They are called 'dummies' because the numbers themselves don't have inherent meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy = pd.get_dummies(df_explored)\n",
    "\n",
    "df_dummy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data into training and testing data**\n",
    "\n",
    "An important step in testing a model is to split data into training and testing data. We will put the target data Severity in a separate dataframe and drop price data in another separate x data data frame for our predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = df_dummy['SEVERITYCODE']\n",
    "x_data=df_dummy.drop('SEVERITYCODE',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)\n",
    "\n",
    "\n",
    "print(\"number of test samples :\", x_test.shape[0])\n",
    "print(\"number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scope of this Capstone is to create predictions for accident severity using Supervised Machine Learning.\n",
    "\n",
    "In general, this gives us two algorithm choices: Regression and classification algorithms. So which one would be suitable for our problem at hand?\n",
    "\n",
    "**Regression algorithms**\n",
    "\n",
    "In supervised machine learning, regression algorithms attempt to estimate the mapping function from the input variables to numerical or continuous output variables.\n",
    "\n",
    "With regression algorithms, y is a real value, which can be an integer or a float. \n",
    "\n",
    "For example, when provided with a dataset about cars, and you are asked to predict their prices, that is a regression task because price will be a continuous output.\n",
    "\n",
    "An example of a common regression algorithm would be linear regression.\n",
    "\n",
    "**Classification algorithms**\n",
    "\n",
    "In supervised machine learning, classification algorithms attempt to estimate the mapping function from the input variables to discrete or categorical output variables.\n",
    "\n",
    "With classification algorithms, y is a category that the mapping function predicts. \n",
    "\n",
    "For example, when provided with a dataset about cars, a classification algorithm can try to predict whether the prices for the cars sell for a low, high or medium price compared to the recommended retail price. So the price will be classified into three discrete categories\n",
    "\n",
    "Examples of the common classification algorithms include logistic regression, Naïve Bayes, decision trees, and K Nearest Neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we have a regression or classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we are measuring severity using numbers, the problem we are facing can only be identified as a classification problem, because the numerical values are discrete and not continuous.\n",
    "\n",
    "In order to classify as continuous, data needs to be able to have almost any numeric value and can be meaningfully subdivided into finer and finer increments, depending upon the precision of the measurement system. This is not the case for Severity.\n",
    "\n",
    "That is why we will focus on the most common classification algorithms for our problem: Naive Bayes, Logistic Regression, K-nearest neighbors and decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of classification algorithms\n",
    "list_alg=['Naive Bayes','Logistic Regression','K-nearest Neighbors','Decision Trees']\n",
    "\n",
    "#List of model metrics\n",
    "list_metric=['Accuracy Score','Logistic Loss']\n",
    "\n",
    "# Initialize an empty list for the accuracy score for each algorithm\n",
    "accuracy_lst=[]\n",
    "\n",
    "# Initialize an empty list for the log loss for each algorithm\n",
    "log_loss_lst=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(x_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_hat = gnb.predict(x_test)\n",
    "\n",
    "# Get the accuracy score\n",
    "acc=accuracy_score(y_test, y_hat)\n",
    "\n",
    "# Append to the accuracy list\n",
    "accuracy_lst.append(acc)\n",
    "\n",
    "# Get the log loss\n",
    "gnb_probas = gnb.predict_proba(x_test)\n",
    "gnb_log_loss = log_loss(y_test, gnb_probas)\n",
    "\n",
    "# Append to the log loss list\n",
    "log_loss_lst.append(gnb_log_loss)\n",
    "\n",
    "\n",
    "print(\"[Naive Bayes] accuracy_score: {:.3f}.\".format(acc))\n",
    "print(\"[Naive Bayes] log_loss: {:.3f}.\".format(gnb_log_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "logr = LogisticRegression(random_state=0)\n",
    "logr.fit(x_train,y_train)\n",
    "y_hat =logr.predict(x_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_hat))\n",
    "\n",
    "# Get the accuracy score\n",
    "acc=accuracy_score(y_test, y_hat)\n",
    "\n",
    "# Append to the accuracy list\n",
    "accuracy_lst.append(acc)\n",
    "\n",
    "# Get the log loss\n",
    "logr_probas = logr.predict_proba(x_test)\n",
    "logr_log_loss = log_loss(y_test, logr_probas)\n",
    "\n",
    "# Append to the log loss list\n",
    "log_loss_lst.append(logr_log_loss)\n",
    "\n",
    "\n",
    "\n",
    "print(\"[Logistic regression algorithm] accuracy_score: {:.3f}.\".format(acc))\n",
    "print(\"[Logistic regression algorithm] log_loss: {:.3f}.\".format(logr_log_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3: K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-NN classifier with 6 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(x_train,y_train)\n",
    "\n",
    "# Predict the labels for the training data X\n",
    "y_hat = knn.predict(x_test)\n",
    "\n",
    "# Get the accuracy score\n",
    "acc=accuracy_score(y_test, y_hat)\n",
    "\n",
    "# Append to the accuracy list\n",
    "accuracy_lst.append(acc)\n",
    "\n",
    "# Get the log loss\n",
    "knn_probas = knn.predict_proba(x_test)\n",
    "knn_log_loss = log_loss(y_test, knn_probas)\n",
    "\n",
    "# Append to the log loss list\n",
    "log_loss_lst.append(knn_log_loss)\n",
    "\n",
    "\n",
    "\n",
    "print(\"[KNN] accuracy_score: {:.3f}.\".format(acc))\n",
    "print(\"[KNN] log_loss: {:.3f}.\".format(knn_log_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 4: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decision tree algorithm\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=8, random_state=1)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "# Use dt to predict test set labels\n",
    "y_hat= dt.predict(x_test)\n",
    "\n",
    "# Get the accuracy score\n",
    "acc=accuracy_score(y_test, y_hat)\n",
    "\n",
    "# Append to the accuracy list\n",
    "accuracy_lst.append(acc)\n",
    "\n",
    "# Get the log loss\n",
    "dt_probas = dt.predict_proba(x_test)\n",
    "dt_log_loss = log_loss(y_test, dt_probas)\n",
    "\n",
    "# Append to the log loss list\n",
    "log_loss_lst.append(dt_log_loss)\n",
    "\n",
    "\n",
    "print(\"[Decision tree] accuracy_score: {:.3f}.\".format(acc))\n",
    "print(\"[Decision tree] log_loss: {:.3f}.\".format(dt_log_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy scores and logistic loss for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling DataFrame constructor after zipping accuracy score and log_loss lists, with columns specified for each algorithm\n",
    "df_metric = pd.DataFrame(list(zip(accuracy_lst, log_loss_lst)), columns =list_metric, index=list_alg) \n",
    "df_metric "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the models I built, the Decision Trees model performed the best (~98.3% accuracy, ~0.071 logarithmic loss), and Logistic Regression model performed almost equally good. In general the performance differences between models were minor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion <a name=\"discussion\"></a>\n",
    "\n",
    "\n",
    "Even though I was able to achieve a high accuracy score for the models, I think they still leave a lot of room for improvement.\n",
    "For example, the chosen dataset was very feature rich, most features did not have a good correlation with the value we wanted to predict. So one option would be to combine the dataset with additional collision data for Seattle, to get more correlated features that can be used in the prediction models.\n",
    "In terms of usefulness of the predicted metric, it would be good to not only consider injuries and fatalities when looking at severity, but also look at the impact the collision has on the traffic, e.g. traffic delays for other commuters.\n",
    "Finally, it would be nice to improve the model by not only predicting the severity of an accident, but to also predict whether or not a person might be involved in an accident when driving a certain route at a certain time and what severity that accident might have.\n",
    "\n",
    "\n",
    "## 5. Conclusion <a name=\"conclusion\"></a>\n",
    "\n",
    "In this notebook, I analyzed the relationship between a collision severity and other characteristic of the collision. I identified injuries, address type, pedestrian right of way granted and hit parked car as the most important features that allow us to predict the severity. I built classification models to predict the severity of an accident. These models can be very useful in commuters to drive with more foresight or change their travel plans accordingly when a collision occurs on their way to work. In addition, the prediction model can help insurance companies predict possible costs for accidents suffered by their members."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
